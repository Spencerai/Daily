# Daily 何红亮每日学习“日记”  2019

    这是何红亮同学保存每日记录的地方，GitHub上的第一个目录。</p>
    在这里，我将记录每天自己看到的有收获的文章。比如：数据挖掘、机器学习、深度学习、NLP、数据竞赛、Python进阶及有关数据及建模方面内容。
    在数据科学的道路上，能够慢慢进步和成长！坚持每天进步一点点！</p>
    如有小伙伴愿意在数据道路上结伴而行，可以关注微信公众号《通往数据自由之路》在后台留言。
    <br>Keywords: data mining  machine learning  deep learning（数据挖掘，机器学习，深度学习，自然语言处理 NLP）
----
<br>

以下是2019年目录！<br>


20190121:<br>
ICLR2019少样本学习新思路：利用转导(Transductive)和标签传播
>https://zhuanlan.zhihu.com/p/55111343



20190120:<br>
免费自然语言处理(NLP)课程及教材分享
>https://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247485447&idx=1&sn=93deeaeb758c78af6a06e9d4efa79379



20190119:<br>
详解谷歌最强NLP模型BERT（理论+实战）
>https://mp.weixin.qq.com/s?__biz=MzU1NTUxNTM0Mg==&mid=2247490161&idx=3&sn=25c3b79f79bc7de573dabf695b7ce36f




20190118:<br>
赛尔原创 | 反讽识别综述
>https://zhuanlan.zhihu.com/p/55212330

文本分类实战--从TFIDF到深度学习（附代码）
>https://blog.csdn.net/liuchonge/article/details/72614524




20190117:<br>
大众点评搜索基于知识图谱的深度学习排序实践
>https://tech.meituan.com/2019/01/17/dianping-search-deeplearning.html




20190116:<br>
Attention mechanisms on NLP
>https://zhuanlan.zhihu.com/p/54491016


20190115:<br>
不只有BERT！盘点2018年NLP令人激动的10大想法
>https://zhuanlan.zhihu.com/p/53009094

NLP预训练模型大集合！
>https://zhuanlan.zhihu.com/p/53569058





20190114:<br>
以虎嗅网4W+文章的文本挖掘为例，展现数据分析的一整套流程
>https://zhuanlan.zhihu.com/p/52782063


20190113:<br>
放弃幻想，全面拥抱Transformer：自然语言处理三大特征抽取器（CNN/RNN/TF）比较（作者：张俊林）
>https://zhuanlan.zhihu.com/p/54743941


20190112:<br>
换个角度看GAN：另一种损失函数
>https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754873&idx=3&sn=d646367b6abefd713a6c6d9d2630b2fb



20190111:<br>
如何用Keras构建一个Encoder-Decoder模型
>https://zhuanlan.zhihu.com/p/54657160



20190110:<br>
深度CTR预估模型中的特征自动组合机制演化简史
>https://zhuanlan.zhihu.com/p/52876883



20190109:<br>
研究综述 | 事件抽取及推理 (上)
>https://mp.weixin.qq.com/s?__biz=MzI0MTI1Nzk1MA==&mid=2651676681&idx=1&sn=42caea104b2e5df59710c2fea77dae86

研究综述 | 事件抽取及推理 (下)
>https://mp.weixin.qq.com/s?__biz=MzI0MTI1Nzk1MA==&mid=2651676727&idx=1&sn=a8d1692583133b752f06699f1c0790c2



20190108:<br>
图神经网络概述第三弹：来自IEEE Fellow的GNN综述
>https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650755237&idx=1&sn=2dd0468552e69057681eec58fd265cbb


20190107:<br>
初学者如何避免在序列预测问题中遇到的陷阱？
>https://mp.weixin.qq.com/s?__biz=MzU1MTkwNzIyOQ==&mid=2247486961&idx=1&sn=2fbf7eb3588b7322bf3e12d412ad8fd0

遗珠之作？谷歌Quoc Le这篇NLP预训练模型论文值得一看
>https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650755237&idx=4&sn=9f55bcfcb73fcac5541e922278e38624



20190106:<br>
2018年，NLP研究与应用进展到什么水平了？
>https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247494662&idx=1&sn=54462c8100bbbf6bd1d99f12bb0be3df

一文概述 2018 年深度学习 NLP 十大创新思路
>https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&mid=2247493804&idx=1&sn=c597e5026edd3f98b85b9f2a6f2dcb39





20190105:<br>
你已经是个成熟的表格，该学会NLP了
>https://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&mid=2247500854&idx=2&sn=1c95e13dc948a626b9ffa39a91d058e0



20190104:<br>
从信息瓶颈理论一瞥机器学习的“大一统理论”
>https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247494028&idx=1&sn=75c2d19e68f919572fb86086d6e9b19d




20190103:<br>
论文浅尝 | 提取计数量词丰富知识库
>https://mp.weixin.qq.com/s?__biz=MzU2NjAxNDYwMg==&mid=2247485346&idx=1&sn=9ca9721b81b56c7113babd27e0fd0208





20190102:<br>
「回顾」机器学习与推荐系统实践
>https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247487291&idx=1&sn=53b384335cb124da689bc3e259f45f46




20190101:<br>
ELMo的朋友圈：预训练语言模型真的一枝独秀吗？
>https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754873&idx=2&sn=15c725776d9345ad12f5d1a50ac8e9e2

